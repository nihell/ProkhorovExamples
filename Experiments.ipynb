{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15961f45-3f99-49c8-9d53-f0bf5f937ec9",
   "metadata": {},
   "source": [
    "### Experiments for Prokhorov Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcfb638e-abe0-4ba4-bdfc-8419e4d17ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import gudhi as gd\n",
    "from gudhi import hera\n",
    "from gudhi import representations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn import manifold\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy.optimize as spo \n",
    "from scipy.optimize import minimize\n",
    "import kmedoids\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import itertools\n",
    "from utils import run_single_fold\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5da1a-42b2-4774-9937-0cea5a4501bd",
   "metadata": {},
   "source": [
    "Some functions to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1be77b-088c-45d4-9189-344820893008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diag_to_array(data):\n",
    "    dataset, num_diag = [], len(data[\"0\"].keys())\n",
    "    for dim in data.keys():\n",
    "        X = []\n",
    "        for diag in range(num_diag):\n",
    "            pers_diag = np.array(data[dim][str(diag)])\n",
    "            X.append(pers_diag)\n",
    "        dataset.append(X)\n",
    "    return dataset\n",
    "\n",
    "def diag_to_dict(D):\n",
    "    X = dict()\n",
    "    for f in D.keys():\n",
    "        df = diag_to_array(D[f])\n",
    "        for dim in range(len(df)):\n",
    "            X[str(dim) + \"_\" + f] = df[dim]\n",
    "    return X \n",
    "\n",
    "def load_PI_data(dim = \"1\", noise = \"1\"):\n",
    "    path = \"../PersistenceImages/matlab_code/sixShapeClasses/ToyData_PD_TextFiles/\"\n",
    "    PDs = {}\n",
    "    for entry in os.scandir(path):\n",
    "        if(entry.name.endswith(\".txt\")):\n",
    "            PDs[entry.name] = {\"noise\":entry.name.split(sep=\"_\")[2],\n",
    "                               \"index\":entry.name.split(sep=\"_\")[3],\n",
    "                               \"shape_class\":entry.name.split(sep=\"_\")[4],\n",
    "                               \"dimension\":entry.name.split(sep=\"_\")[5],\n",
    "                               \"data\":np.loadtxt(entry.path)}\n",
    "    persistence_list = np.array([pd[\"data\"] for pd in PDs.values() if(pd[\"noise\"]==\"n{}\".format(noise) and pd[\"dimension\"]==\"{}.txt\".format(dim))])\n",
    "    labels = np.array([int(pd[\"shape_class\"]) for pd in PDs.values() if(pd[\"noise\"]==\"n{}\".format(noise) and pd[\"dimension\"]==\"{}.txt\".format(dim))], dtype = int)\n",
    "    return persistence_list, labels\n",
    "\n",
    "def load_3d_data():\n",
    "    path = \"../sklearn-tda/example/3DSeg/\"\n",
    "    train_lab  = pd.read_csv(path+\"train.csv\")\n",
    "    train_diag = diag_to_dict(h5py.File(path+\"train_diag.hdf5\", \"r\"))\n",
    "    return np.array(train_diag[\"1_geodesic\"]), np.array(train_lab[\"part\"], dtype=int)\n",
    "\n",
    "def load_cubical_data(filt=\"upper\", d=1):\n",
    "    f= h5py.File('cubical_data_{}_{}d.hdf5'.format(filt,d), \"r\")[\"list_of_lists\"]\n",
    "    X = []\n",
    "    for i in range(0,len(f)):\n",
    "        X.append(np.array(f[str(i)]))\n",
    "    y = np.loadtxt('cubical_labels_{}_{}d.txt'.format(filt,d), dtype = int)\n",
    "    return np.array(X),y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e94bb247-cc99-427c-9d5a-d80e25927685",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61737/1933722931.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(train_diag[\"1_geodesic\"]), np.array(train_lab[\"part\"], dtype=int)\n"
     ]
    }
   ],
   "source": [
    "#X,y = load_PI_data(dim=\"0\", noise=\"1\")\n",
    "#X,y = load_cubical_data(filt = \"lower\", d=0)\n",
    "X,y = load_3d_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd0adf36-5a60-4cea-88fc-c2ebea386a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5700,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d98e647-36a9-4209-bc77-5e1d647d9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae33834-2146-477c-8762-1a66653980ed",
   "metadata": {},
   "source": [
    "## K-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1550a300-389e-495b-bc92-b9ac7c31cc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric = \"precomputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861622c3-31b8-4bf6-b645-1d6512b35a9f",
   "metadata": {},
   "source": [
    "Perform a grid search with cross-validation among integer slopes for the Prokhorov parameter function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b5d0309-c173-46bb-8bd7-ba669b9c2e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55506b8d2ed44f49817571833c6e52d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search completed in 40440.76908326149s, best parameter is 10 with score 0.9101754385964913\n"
     ]
    }
   ],
   "source": [
    "list_of_params = range(10,100)\n",
    "best_param = 0\n",
    "best_score = 0\n",
    "\n",
    "tic = time.time()\n",
    "for p in tqdm(list_of_params):\n",
    "    PD = gd.representations.ProkhorovDistance(n_jobs = -1, coefs = np.array([0,p]))\n",
    "    PD.fit(X_train)\n",
    "    P_train = PD.transform(X_train)\n",
    "    cv_scores = cross_val_score(knn, P_train, y_train, cv=5)\n",
    "    if np.mean(cv_scores)>best_score:\n",
    "        best_param = p\n",
    "        best_score = np.mean(cv_scores)\n",
    "        \n",
    "print(\"grid search completed in {}s, best parameter is {} with score {}\".format(time.time()-tic, best_param, best_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8cecc03-b8dd-4813-8607-ebb4bbf5b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ProkhorovDistance(coefs=array([ 0, 10]), n_jobs=-1)------------------\n",
      "distance computation done in  795.2427561283112\n",
      "training accuracy:  0.9101754385964913\n",
      "test accuracy:  0.9270175438596491\n",
      "------------------BottleneckDistance(n_jobs=-1)------------------\n",
      "distance computation done in  1252.453706741333\n",
      "training accuracy:  0.9098245614035088\n",
      "test accuracy:  0.9245614035087719\n",
      "------------------WassersteinDistance(internal_p=1, n_jobs=-1)------------------\n",
      "distance computation done in  838.5659482479095\n",
      "training accuracy:  0.9042105263157895\n",
      "test accuracy:  0.9312280701754386\n",
      "------------------WassersteinDistance(internal_p=2, n_jobs=-1, order=2)------------------\n",
      "distance computation done in  1740.7613213062286\n",
      "training accuracy:  0.9059649122807019\n",
      "test accuracy:  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "reps = [gd.representations.ProkhorovDistance(n_jobs=-1, coefs = np.array([0,best_param])),\n",
    "        gd.representations.BottleneckDistance(n_jobs=-1),\n",
    "        gd.representations.WassersteinDistance(n_jobs=-1, order=1, internal_p =1),\n",
    "        gd.representations.WassersteinDistance(n_jobs=-1, order=2, internal_p =2)]\n",
    "\n",
    "for r in reps:\n",
    "    print(\"------------------\"+str(r)+\"------------------\")\n",
    "    tic = time.time()\n",
    "    r.fit(X_train)\n",
    "    D_matrix_train = r.transform(X_train)\n",
    "    D_matrix_test = r.transform(X_test)\n",
    "    print(\"distance computation done in \", time.time()-tic)\n",
    "    cv_scores = cross_val_score(knn, D_matrix_train, y_train, cv=5)\n",
    "    print(\"training accuracy: \",np.mean(cv_scores))\n",
    "    knn.fit(D_matrix_train, y_train)\n",
    "    score = knn.score(D_matrix_test, y_test)\n",
    "    print(\"test accuracy: \",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bee447-be6d-4b6d-8be1-e49d4c01e3c3",
   "metadata": {},
   "source": [
    "## K-Medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b5ff515-ac26-4813-a33c-3164e55e6481",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = kmedoids.KMedoids(n_clusters = len(np.unique(y)), metric=\"precomputed\", init = \"build\", random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2b60ebdd-fb49-4e87-9a78-feb65aa6a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_info(cluster_labels,y_train):\n",
    "    # Initializing\n",
    "    reference_labels = {}\n",
    "    # For loop to run through each label of cluster label\n",
    "    for i in range(len(np.unique(cluster_labels))):\n",
    "        index = np.where(cluster_labels == i,1,0)\n",
    "        num = np.bincount(y_train[index==1]).argmax()\n",
    "        reference_labels[i] = num\n",
    "    return reference_labels\n",
    "\n",
    "def km_cv(km, rep, X, y, cv):\n",
    "    rs = ShuffleSplit(n_splits=cv, test_size=1.0/cv, random_state=0)\n",
    "    scores = []\n",
    "    for train_index, test_index in rs.split(X):\n",
    "        train_index = np.array(train_index)\n",
    "        test_index = np.array(test_index)\n",
    "        X_train_cv = X[train_index]\n",
    "        y_train_cv = y[train_index]\n",
    "        X_test_cv = X[test_index]\n",
    "        y_test_cv = y[test_index]\n",
    "        \n",
    "        rep.fit(X_train_cv)\n",
    "        km.fit(rep.transform(X_train_cv))\n",
    "        label_dictionary = retrieve_info(km.labels_, y_train_cv)\n",
    "        \n",
    "        predicted_clusters = km.predict(rep.transform(X_test_cv))\n",
    "        predicted_labels = [label_dictionary[predicted_clusters[i]] for i in range(0,len(predicted_clusters))]\n",
    "            \n",
    "        scores.append(accuracy_score(predicted_labels,y_test_cv))\n",
    "    \n",
    "    \n",
    "    return scores\n",
    "\n",
    "def km_cv_parallel(km, rep, X, y, cv):\n",
    "    skf = StratifiedKFold(n_splits=cv, random_state=666, shuffle=True)\n",
    "    rep.fit(X)\n",
    "    D_matrix=rep.transform(X)\n",
    "    \n",
    "    NUM_WORKERS = None # if none uses all cores\n",
    "    with ProcessPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "        train_acc_log, valid_acc_log = zip(*executor.map(run_single_fold, \n",
    "                                                         itertools.repeat(D_matrix),\n",
    "                                                         itertools.repeat(y),\n",
    "                                                         [train_index for train_index, _ in skf.split(X, y)],\n",
    "                                                         [valid_index for _, valid_index in skf.split(X, y)]))\n",
    "        \n",
    "    return valid_acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e50bc341-3b4b-4371-8022-121e8c0007d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4b410b0c0f41fd97e660086b55bef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search completed in 41715.83107829094s, best parameter is 13 with score 0.46842105263157896\n"
     ]
    }
   ],
   "source": [
    "list_of_params = range(10,100)\n",
    "best_param = 0\n",
    "best_score = 0\n",
    "\n",
    "tic = time.time()\n",
    "for p in tqdm(list_of_params):\n",
    "    PD = gd.representations.ProkhorovDistance(n_jobs = -1, coefs = np.array([0,p]))\n",
    "\n",
    "    cv_scores = km_cv_parallel(km, PD, X_train, y_train, 5)\n",
    "    if np.mean(cv_scores)>best_score:\n",
    "        best_param = p\n",
    "        best_score = np.mean(cv_scores)\n",
    "        \n",
    "print(\"grid search completed in {}s, best parameter is {} with score {}\".format(time.time()-tic, best_param, best_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eb07c651-43df-479b-a59a-2253f3789ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ProkhorovDistance(coefs=array([ 0, 13]), n_jobs=-1)------------------\n",
      "distance computation done in  1946.679748058319\n",
      "training accuracy:  0.4792982456140351\n",
      "test accuracy:  0.49859649122807015\n",
      "------------------BottleneckDistance(n_jobs=-1)------------------\n",
      "distance computation done in  3467.9827699661255\n",
      "training accuracy:  0.4905263157894737\n",
      "test accuracy:  0.48912280701754385\n",
      "------------------WassersteinDistance(internal_p=1, n_jobs=-1)------------------\n",
      "distance computation done in  2057.5558784008026\n",
      "training accuracy:  0.40210526315789474\n",
      "test accuracy:  0.4126315789473684\n",
      "------------------WassersteinDistance(internal_p=2, n_jobs=-1, order=2)------------------\n",
      "distance computation done in  5009.361773967743\n",
      "training accuracy:  0.4592982456140351\n",
      "test accuracy:  0.45649122807017545\n"
     ]
    }
   ],
   "source": [
    "reps = [gd.representations.ProkhorovDistance(n_jobs=-1, coefs = np.array([0,best_param])),\n",
    "        gd.representations.BottleneckDistance(n_jobs=-1),\n",
    "        gd.representations.WassersteinDistance(n_jobs=-1, order=1, internal_p =1),\n",
    "        gd.representations.WassersteinDistance(n_jobs=-1, order=2, internal_p =2)]\n",
    "\n",
    "for r in reps:\n",
    "    print(\"------------------\"+str(r)+\"------------------\")\n",
    "    tic = time.time()\n",
    "    r.fit(X_train)\n",
    "    D_matrix_train = r.transform(X_train)\n",
    "    r.fit(np.concatenate((X_train,X_test),axis=0))\n",
    "    D_matrix_test = r.transform(np.concatenate((X_train,X_test),axis=0))\n",
    "    print(\"distance computation done in \", time.time()-tic)\n",
    "    #cv_scores = km_cv_parallel(km, r, X_train, y_train, 5)\n",
    "    #print(\"training accuracy: \",np.mean(cv_scores))\n",
    "   \n",
    "\n",
    "    km = kmedoids.KMedoids(n_clusters = len(np.unique(y)), metric=\"precomputed\", init = \"build\", random_state = 42) # why init = 'build' ??\n",
    "    km.fit(D_matrix_train)\n",
    "    km.dict_ = {i: y_train[id] for i, id in enumerate(km.medoid_indices_)}\n",
    "\n",
    "    predicted_train_labels = [km.dict_[id] for id in km.predict(D_matrix_train)]\n",
    "    train_acc = accuracy_score(predicted_train_labels, y_train)\n",
    "    print(\"training accuracy: \",train_acc)\n",
    "    \n",
    "    predicted_valid_labels = [km.dict_[id] for id in km.predict(D_matrix_test)][len(y_train):] # we only look at the labels for the new data\n",
    "    valid_acc = accuracy_score(predicted_valid_labels, y_test) \n",
    "    print(\"test accuracy: \",valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3c2cb-8270-483c-81a4-58f11641c719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
