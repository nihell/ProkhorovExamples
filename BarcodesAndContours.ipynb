{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import gudhi as gd\n",
    "from gudhi import hera\n",
    "from gudhi import representations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import sklearn\n",
    "from sklearn import manifold\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scipy.optimize as spo \n",
    "from scipy.optimize import minimize\n",
    "import kmedoids\n",
    "import networkx as nx\n",
    "from networkx import bipartite\n",
    "import persim\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def diag_to_array(data):\n",
    "    dataset, num_diag = [], len(data[\"0\"].keys())\n",
    "    for dim in data.keys():\n",
    "        X = []\n",
    "        for diag in range(num_diag):\n",
    "            pers_diag = np.array(data[dim][str(diag)])\n",
    "            X.append(pers_diag)\n",
    "        dataset.append(X)\n",
    "    return dataset\n",
    "\n",
    "def diag_to_dict(D):\n",
    "    X = dict()\n",
    "    for f in D.keys():\n",
    "        df = diag_to_array(D[f])\n",
    "        for dim in range(len(df)):\n",
    "            X[str(dim) + \"_\" + f] = df[dim]\n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../sklearn-tda/example/3DSeg/\"\n",
    "train_lab  = pd.read_csv(path+\"train.csv\")\n",
    "train_diag = diag_to_dict(h5py.File(path+\"train_diag.hdf5\", \"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train points = 285\n",
      "Number of test  points = 5415\n",
      "[1436  748 4596 ... 5226 5390  860]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-1108d7dfc666>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  limit                = np.int(test_size * train_num_pts)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Size of test set\n",
    "test_size = 0.95\n",
    "\n",
    "# Shuffle dataset and pick points for test set\n",
    "train_num_pts        = train_lab.shape[0]    \n",
    "perm                 = np.random.RandomState(seed=42).permutation(train_num_pts)\n",
    "limit                = np.int(test_size * train_num_pts)\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "\n",
    "# Create train and test labels with LabelEncoder from scikit-learn\n",
    "train_full_labels  = train_lab[\"part\"]\n",
    "le                 = LabelEncoder()\n",
    "train_labels       = np.array(le.fit_transform(train_full_labels[train_sub]))\n",
    "test_labels        = np.array(le.transform(train_full_labels[test_sub]))\n",
    "\n",
    "# Create train and test sets of persistence diagrams\n",
    "train_full_diag    = train_diag[\"1_geodesic\"]\n",
    "train_diag         = [train_full_diag[i] for i in train_sub]\n",
    "test_diag          = [train_full_diag[i] for i in test_sub]\n",
    "\n",
    "# Print sizes\n",
    "train_num_pts, test_num_pts = len(train_sub), len(test_sub)\n",
    "print(\"Number of train points = \" + str(train_num_pts))\n",
    "print(\"Number of test  points = \" + str(test_num_pts))\n",
    "print(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 3, 1, 0, 1, 3, 0, 0, 0, 0, 0, 0, 3, 0, 3, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 3, 0, 1, 0, 0, 0, 1, 3, 1, 0, 0, 1, 1, 1, 2, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 2, 3, 1, 0, 1, 1, 0, 1, 0, 0, 0, 2, 0, 1, 3, 1, 3, 3, 0,\n",
       "       0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 3, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 3, 0, 1, 1, 3, 0, 2, 1, 0, 3, 0,\n",
       "       1, 3, 0, 0, 0, 3, 0, 1, 2, 1, 3, 0, 0, 1, 0, 0, 2, 1, 1, 1, 1, 3,\n",
       "       0, 1, 2, 3, 0, 1, 0, 2, 0, 0, 2, 0, 3, 1, 1, 0, 1, 0, 0, 2, 1, 3,\n",
       "       1, 2, 2, 0, 0, 0, 0, 0, 1, 3, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0,\n",
       "       1, 0, 3, 2, 2, 1, 3, 1, 0, 0, 1, 1, 1, 0, 0, 3, 3, 0, 1, 3, 3, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 0, 2, 3, 3, 0, 0, 3, 1, 1,\n",
       "       0, 1, 0, 0, 0, 1, 3, 1, 1, 1, 3, 0, 1, 0, 1, 3, 1, 1, 0, 3, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 2, 1, 0, 2, 1, 1, 3, 3, 1, 1, 0, 3, 1, 0, 1, 0,\n",
       "       0, 0, 1, 3, 0, 0, 1, 0, 3, 3, 3, 3, 1, 1, 1, 1, 0, 1, 3, 0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0=[(0,tuple(pt)) for pt in train_diag[0]]\n",
    "D1=[(1,tuple(pt)) for pt in train_diag[2]]\n",
    "D01 = D0+D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9519f834b5d740b88c8973108bd91e39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1,ax2) = plt.subplots(1,2, figsize = (15,10))\n",
    "gd.plot_persistence_barcode(D01, axes=ax1)\n",
    "#gd.plot_persistence_diagram(D01, axes=ax2)\n",
    "db, (m, D) = persim.bottleneck(train_diag[0],train_diag[2], matching=True)\n",
    "persim.wasserstein_matching(train_diag[0],train_diag[2], m, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cospan_from_matching(X, Y, m):\n",
    "    D1 = X\n",
    "    D2 = Y\n",
    "    for y in Y:\n",
    "        D1 = np.append(D1, np.array([[(y[0]+y[1])/2,(y[0]+y[1])/2]]), axis=0)\n",
    "    for x in X:\n",
    "        D2 = np.append(D2, np.array([[(x[0]+x[1])/2,(x[0]+x[1])/2]]), axis=0)\n",
    "    Clist = []\n",
    "    for i,j in m:\n",
    "        Clist.append([min(D1[i][0],D2[j][0]),min(D1[i][1],D2[j][1])])\n",
    "    C = np.array(Clist)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f782a76dcaf4195b40d560d10fd500e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f,ax = plt.subplots(1,1)\n",
    "A = train_diag[0]\n",
    "B = train_diag[1]\n",
    "db, (m, D) = persim.bottleneck(A,B, matching=True)\n",
    "C=cospan_from_matching(A, B, m)\n",
    "persim.plot_diagrams([A, B, C])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D(r_vals, t):\n",
    "    return np.min(np.where(np.array(r_vals)<=t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce53220f182423e8680053668759b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r_vals = []\n",
    "n_points=len(train_diag[0])+len(train_diag[1])\n",
    "#compute the D function\n",
    "d_vals = np.linspace(0,n_points,n_points+1)\n",
    "r_vals = [gd.prokhorov_distance(train_diag[0], train_diag[1], np.array([r])) for r in d_vals]\n",
    "f,ax = plt.subplots(1,1)\n",
    "for i in range(1,n_points+1):\n",
    "    ax.plot((r_vals[i],r_vals[i-1]),(i, i), c='tab:orange', ls='-', linewidth = 1.0, label = '$D_{X,Y}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(train_diag[0]))\n",
    "print(len(train_diag[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prokhorov(I1, I2, matching=False):\n",
    "    g = nx.Graph()\n",
    "    #add nodes for points\n",
    "    g.add_nodes_from(list(zip(range(0,len(I1)),[0]*len(I1))), bipartite=0)\n",
    "    g.add_nodes_from(list(zip(range(0,len(I2)),[1]*len(I2))), bipartite=1)\n",
    "    #add all edges with cost = max.norm\n",
    "    for v in [n for n, d in g.nodes(data=True) if d['bipartite']==0]:\n",
    "        for w in [n for n, d in g.nodes(data=True) if d['bipartite']==1]:\n",
    "            cost = max(abs(I1[v[0],0]-I2[w[0],0]),abs(I1[v[0],1]-I2[w[0],1]))#/2#*(1-I1[v[0],1]+I1[v[0],0])*(1-I2[w[0],1]+I2[w[0],0])\n",
    "            #print(cost)\n",
    "            g.add_edge(v,w, weight=cost)#nudge towards matching off-diagonal points?\n",
    "\n",
    "    #add points on the diagonal\n",
    "    g.add_nodes_from(list(zip(range(len(I1),len(I1)+len(I2)),[0]*len(I2))), bipartite=0)\n",
    "    g.add_nodes_from(list(zip(range(len(I2),len(I2)+len(I1)),[1]*len(I1))), bipartite=1)\n",
    "    #add edges to the diagonal with cost = max.norm\n",
    "    for v in range(0,len(I1)):\n",
    "        cost = (I1[v,1]-I1[v,0])/2\n",
    "        g.add_edge((v,0),(len(I2)+v,1), weight=cost)\n",
    "    for v in range(0,len(I2)):\n",
    "        cost = (I2[v,1]-I2[v,0])/2\n",
    "        g.add_edge((v,1),(len(I1)+v,0), weight=cost)\n",
    "    for w in range(0,len(I1)):\n",
    "        for v in range(0,len(I2)):\n",
    "            g.add_edge((len(I1)+v,0), (len(I2)+w,1),weight = 0)\n",
    "\n",
    "    #Get weights and sort them\n",
    "    weights = nx.get_edge_attributes(g, \"weight\")\n",
    "    #sorted_weights = np.sort(np.fromiter(weights.values(),dtype=float))\n",
    "\n",
    "    #perform binary search\n",
    "    L=0\n",
    "    R=max(weights.values())#len(sorted_weights)-1\n",
    "    print(max(weights.values()))\n",
    "    eps = 0\n",
    "    match = None\n",
    "    while(R-L>0.00001):\n",
    "        #m = \n",
    "        old_eps=eps\n",
    "        eps = (L+R)/2\n",
    "        geps=nx.create_empty_copy(g)\n",
    "        for v in [n for n, d in g.nodes(data=True) if d['bipartite']==0]:\n",
    "            for w in [n for n, d in g.nodes(data=True) if d['bipartite']==1]:\n",
    "                if (v,w) in g.edges():\n",
    "                    if (v,w) in weights:\n",
    "                        if weights[(v,w)]<=eps:\n",
    "                            geps.add_edge(v,w)\n",
    "                    else:\n",
    "                        if weights[(w,v)]<=eps:\n",
    "                            geps.add_edge(v,w)\n",
    "        #Compute matching on subgraph geps\n",
    "        old_match = match\n",
    "        match = nx.bipartite.hopcroft_karp_matching(geps,[n for n, d in g.nodes(data=True) if d['bipartite']==0])\n",
    "        #Compute the amount of unmatched points\n",
    "        match = [(x[0],y[0]) for x,y in match.items() if x[1]==0]\n",
    "        number_of_unmatched_points = len(I1)+len(I2)-len(match)\n",
    "        \n",
    "        #print(\"for t = {} the matching has size {}\".format(eps, len(match)))\n",
    "        \n",
    "        if number_of_unmatched_points > eps:\n",
    "            L = eps\n",
    "        elif number_of_unmatched_points < eps:\n",
    "            R = eps\n",
    "        \n",
    "        #if R-L <=1:\n",
    "         #   if R==L:\n",
    "          #      break\n",
    "           # else:\n",
    "            #    if number_of_unmatched_points > eps:\n",
    "             #       L=R\n",
    "              #  elif number_of_unmatched_points < eps:\n",
    "               #     R=L\n",
    "        #else:\n",
    "         #   if number_of_unmatched_points > eps:\n",
    "          #      L = m\n",
    "           # elif number_of_unmatched_points < eps:\n",
    "            #    R = m\n",
    "            #else:\n",
    "             #   break\n",
    "    \n",
    "    dist = old_eps#min(amount, sorted_weights[m+1]) if m > 0 else eps\n",
    "    \n",
    "    if matching:\n",
    "        return dist, old_match\n",
    "    else:\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.483767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12913002143096924"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prokhorov(train_diag[0],train_diag[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
