{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Topological Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathieu Carrière, https://mathieucarriere.github.io/website/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gudhi as gd\n",
    "import gudhi.representations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will see how to efficiently combine machine learning and topological data analysis with the Gudhi library and its [representations](https://gudhi.inria.fr/python/3.1.0.rc1/representations.html) module. We will see how to compute the various Hilbert representations of persistence diagrams and how to use them in order to classify a set of persistence diagrams! Ready? Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate persistence diagrams with orbits of dynamical systems. This dataset is very common in TDA and was introduced in the [persistence image](https://arxiv.org/abs/1507.06217) paper. We use the following system, which depends on a parameter $r>0$:\n",
    "\n",
    "$$x_{n+1}=x_n+ry_n(1-y_n)\\ \\ \\ \\ \\text{(mod 1)}$$\n",
    "$$y_{n+1}=y_n+rx_{n+1}(1-x_{n+1})\\ \\ \\ \\ \\text{(mod 1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see what the point cloud looks like for a given choice of $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = 1000\n",
    "r       = 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we generate the point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.empty([num_pts,2])\n",
    "x, y = np.random.uniform(), np.random.uniform()\n",
    "for i in range(num_pts):\n",
    "    X[i,:] = [x, y]\n",
    "    x = (X[i,0] + r * X[i,1] * (1-X[i,1])) % 1.\n",
    "    y = (X[i,1] + r * x * (1-x)) % 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c364d50ce7664de19bc664b0c8265d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,0], X[:,1], s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mmmh, looks like a random point cloud... We will see later on that interesting topology can appear for some specific values of $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easy way to generate persistence diagrams from this cloud is by using alpha filtrations. This can be done in two lines with Gudhi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "acX = gd.AlphaComplex(points=X).create_simplex_tree()\n",
    "dgmX = acX.persistence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also easily visualize the persistence diagram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1198d080c82742d0a6a0ec75f066ba30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Persistence diagram'}, xlabel='Birth', ylabel='Death'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.plot_persistence_diagram(dgmX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what Gudhi has to offer to turn this diagram into a vector in a [scikit-learn](https://scikit-learn.org/stable/) fashion, that is, with estimators that have fit(), transform(), and fit_transform() methods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first method that was introduced historically is the [persistence landscape]( http://jmlr.org/papers/v16/bubenik15a.html). A persistence landscape is basically obtained by rotating the persistence diagram by $-\\pi/4$ (so that the diagonal becomes the $x$-axis), and then putting tent functions on each point. The $k$th landscape is then defined as the $k$th largest value among all these tent functions. It is eventually turned into a vector by evaluating it on a bunch of uniformly sampled points on the x-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LS = gd.representations.Landscape(resolution=1000)\n",
    "L = LS.fit_transform([acX.persistence_intervals_in_dimension(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L[0][:1000])\n",
    "plt.plot(L[0][1000:2000])\n",
    "plt.plot(L[0][2000:3000])\n",
    "plt.title(\"Landscape\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation, called the [silhouette](https://arxiv.org/abs/1312.0308), takes a weighted average of these tent functions instead. Here, we weight each tent function by the distance of the corresponding point to the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SH = gd.representations.Silhouette(resolution=1000, weight=lambda x: np.power(x[1]-x[0],1))\n",
    "sh = SH.fit_transform([acX.persistence_intervals_in_dimension(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Silhouette')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(sh[0])\n",
    "plt.title(\"Silhouette\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is the [persistence image](http://jmlr.org/papers/v18/16-337.html). A persistence image is obtained by rotating by $-\\pi/4$, centering Gaussian functions on all diagram points (usually weighted by a parameter function---here we consider the squared distance to the diagonal) and summing all these Gaussians. This gives a 2D function, that is pixelized into an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI = gd.representations.PersistenceImage(bandwidth=1e-4, weight=lambda x: x[1]**2, \\\n",
    "                                         im_range=[0,.004,0,.004], resolution=[100,100])\n",
    "pi = PI.fit_transform([acX.persistence_intervals_in_dimension(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b79d6408867481f8c304896b7935d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Persistence Image')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "plt.imshow(np.flip(np.reshape(pi[0], [100,100]), 0))\n",
    "plt.title(\"Persistence Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat, right? Gudhi also contains implementations of less common vectorization methods, such as the [Betti curve](https://www.researchgate.net/publication/316604237_Time_Series_Classification_via_Topological_Data_Analysis), the [complex polynomial]( https://link.springer.com/chapter/10.1007%2F978-3-319-23231-7_27), or the [topological vector](https://diglib.eg.org/handle/10.1111/cgf12692). You can check the [representations](https://gudhi.inria.fr/python/3.1.0.rc1/representations.html) module to check everything that is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gudhi also contain implementations of various kernels, i.e., scalar products for persistence diagrams. More precisely a kernel is a function $k$ that takes a pair of diagrams as inputs and outputs a real value such that:\n",
    "$$k(D,D')=\\langle \\Phi(D), \\Phi(D')\\rangle_{\\mathcal{H}},$$\n",
    "for some implicit Hilbert space $\\mathcal{H}$ and continuous function $\\Phi:\\mathcal{D}\\rightarrow\\mathcal{H}$. Many algorithms, such as SVM or PCA, only require pairwise scalar products to be able to run on data. With Gudhi, you can compute the [persistence scale space kernel](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Reininghaus_A_Stable_Multi-Scale_2015_CVPR_paper.pdf), [persistence weighted Gaussian kernel](http://proceedings.mlr.press/v48/kusano16.html), [persistence Fisher kernel](https://papers.nips.cc/paper/8205-persistence-fisher-kernel-a-riemannian-manifold-kernel-for-persistence-diagrams) and [sliced Wasserstein kernel]( http://proceedings.mlr.press/v70/carriere17a.html). Of course, you can also directly compute the bottleneck and Wasserstein distances ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test these functions, we need a second point cloud and corresponding persistence diagram. Let's pick another $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 4.1\n",
    "Y = np.empty([num_pts,2])\n",
    "x, y = np.random.uniform(), np.random.uniform()\n",
    "for i in range(num_pts):\n",
    "    Y[i,:] = [x, y]\n",
    "    x = (Y[i,0] + r * Y[i,1] * (1-Y[i,1])) % 1.\n",
    "    y = (Y[i,1] + r * x * (1-x)) % 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "acY = gd.AlphaComplex(points=Y).create_simplex_tree()\n",
    "dgmY = acY.persistence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bce199e9114dddb61a2737e0e84550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Persistence diagram'}, xlabel='Birth', ylabel='Death'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.plot_persistence_diagram(dgmY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like this one has interesting homology! Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[:,0], Y[:,1], s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there is a hole in the middle now for some reasons..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check all pairwise kernels and metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PWG kernel is 12.054811785886557\n",
      "PSS kernel is 0.004833343587350214\n",
      "PF kernel is 0.4976412758565667\n",
      "SW kernel is 0.8130229554212283\n",
      "Bottleneck distance is 0.010499971821259642\n",
      "Prokhorov distance is 0.011240616381713645\n",
      "Wasserstein distance is 0.015995267701171198\n"
     ]
    }
   ],
   "source": [
    "PWG = gd.representations.PersistenceWeightedGaussianKernel(bandwidth=0.01, kernel_approx=None,\\\n",
    "                                        weight=lambda x: np.arctan(np.power(x[1], 1)))\n",
    "PWG.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "pwg = PWG.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"PWG kernel is \" + str(pwg[0][0]))\n",
    "\n",
    "PSS = gd.representations.PersistenceScaleSpaceKernel(bandwidth=1.)\n",
    "PSS.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "pss = PSS.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"PSS kernel is \" + str(pss[0][0]))\n",
    "\n",
    "PF = gd.representations.PersistenceFisherKernel(bandwidth_fisher=.001, bandwidth=.001, kernel_approx=None)\n",
    "PF.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "pf = PF.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"PF kernel is \" + str(pf[0][0]))\n",
    "\n",
    "SW = gd.representations.SlicedWassersteinKernel(bandwidth=.1, num_directions=100)\n",
    "SW.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "sw = SW.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"SW kernel is \" + str(sw[0][0]))\n",
    "\n",
    "BD = gd.representations.BottleneckDistance(epsilon=.001)\n",
    "BD.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "bd = BD.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"Bottleneck distance is \" + str(bd[0][0]))\n",
    "\n",
    "PD = gd.representations.ProkhorovDistance(coefs=np.array([0,1]))\n",
    "PD.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "pd = PD.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"Prokhorov distance is \" + str(pd[0][0]))\n",
    "\n",
    "WD = gd.representations.WassersteinDistance(internal_p=2, order=2)\n",
    "WD.fit([acX.persistence_intervals_in_dimension(1)])\n",
    "wd = WD.transform([acY.persistence_intervals_in_dimension(1)])\n",
    "print(\"Wasserstein distance is \" + str(wd[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! However, you have probably noticed that there are quite a lot of parameters to choose. In practice, it is better to cross-validate among a bunch of them and pick the best ones. We will see shortly that it is actually very easy to do this with Gudhi!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a complete dataset. We will generate point clouds and corresponding persistence diagrams for various radii $r$. Of course, you can increase the size of the dataset by modifying the num_diag_per_class variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diag_per_class = 10\n",
    "\n",
    "dgms, labs = [], []\n",
    "for idx, radius in enumerate([2.5, 3.5, 4., 4.1, 4.3]):\n",
    "    for _ in range(num_diag_per_class):\n",
    "        labs.append(idx)\n",
    "        X = np.empty([num_pts,2])\n",
    "        x, y = np.random.uniform(), np.random.uniform()\n",
    "        for i in range(num_pts):\n",
    "            X[i,:] = [x, y]\n",
    "            x = (X[i,0] + radius * X[i,1] * (1-X[i,1])) % 1.\n",
    "            y = (X[i,1] + radius * x * (1-x)) % 1.\n",
    "        ac = gd.AlphaComplex(points=X).create_simplex_tree(max_alpha_square=1e12)\n",
    "        dgm = ac.persistence()\n",
    "        dgms.append(ac.persistence_intervals_in_dimension(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we shuffle the data and create a 80/20 split for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-19dba05446fe>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  limit                = np.int(test_size * len(labs))\n"
     ]
    }
   ],
   "source": [
    "test_size            = 0.2\n",
    "perm                 = np.random.permutation(len(labs))\n",
    "limit                = np.int(test_size * len(labs))\n",
    "test_sub, train_sub  = perm[:limit], perm[limit:]\n",
    "train_labs           = np.array(labs)[train_sub]\n",
    "test_labs            = np.array(labs)[test_sub]\n",
    "train_dgms           = [dgms[i] for i in train_sub]\n",
    "test_dgms            = [dgms[i] for i in test_sub]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to cross-validate among all methods and parameters available, we create a scikit-learn pipeline for processing the diagrams. The pipeline will:\n",
    "1. extract the points of the persistence diagrams with finite coordinates (i.e. the non essential points)\n",
    "2. scale or not the diagrams in the unit square\n",
    "3. handle diagrams with vectorization or kernel methods with Gudhi\n",
    "4. train a classifier from the scikit-learn package\n",
    "\n",
    "As you can see from the code below, it is quite simple! Here, we cross validate among a kernel-SVM with sliced Wasserstein and persistence weighted Gaussian kernels, C-SVM on persistence images, random forests on landscapes, and $k$-nearest neighbors on bottleneck distances. We also try uniformly scaling the diagrams to the unit square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing   import MinMaxScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "from sklearn.svm             import SVC\n",
    "from sklearn.ensemble        import RandomForestClassifier\n",
    "from sklearn.neighbors       import KNeighborsClassifier\n",
    "\n",
    "# Definition of pipeline\n",
    "pipe = Pipeline([(\"Separator\", gd.representations.DiagramSelector(limit=np.inf, point_type=\"finite\")),\n",
    "                 (\"Scaler\",    gd.representations.DiagramScaler(scalers=[([0,1], MinMaxScaler())])),\n",
    "                 (\"TDA\",       gd.representations.PersistenceImage()),\n",
    "                 (\"Estimator\", SVC())])\n",
    "\n",
    "# Parameters of pipeline. This is the place where you specify the methods you want to use to handle diagrams\n",
    "param =    [{\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.SlicedWassersteinKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 1.0],\n",
    "             \"TDA__num_directions\": [20],\n",
    "             \"Estimator\":           [SVC(kernel=\"precomputed\", gamma=\"auto\")]},\n",
    "            \n",
    "            {\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.PersistenceWeightedGaussianKernel()], \n",
    "             \"TDA__bandwidth\":      [0.1, 0.01],\n",
    "             \"TDA__weight\":         [lambda x: np.arctan(x[1]-x[0])], \n",
    "             \"Estimator\":           [SVC(kernel=\"precomputed\", gamma=\"auto\")]},\n",
    "            \n",
    "            {\"Scaler__use\":         [True],\n",
    "             \"TDA\":                 [gd.representations.PersistenceImage()], \n",
    "             \"TDA__resolution\":     [ [5,5], [6,6] ],\n",
    "             \"TDA__bandwidth\":      [0.01, 0.1, 1.0, 10.0],\n",
    "             \"Estimator\":           [SVC()]},\n",
    "            \n",
    "            {\"Scaler__use\":         [True],\n",
    "             \"TDA\":                 [gd.representations.Landscape()], \n",
    "             \"TDA__resolution\":     [100],\n",
    "             \"Estimator\":           [RandomForestClassifier()]},\n",
    "           \n",
    "            {\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.BottleneckDistance()], \n",
    "             \"TDA__epsilon\":        [0.1], \n",
    "             \"Estimator\":           [KNeighborsClassifier(metric=\"precomputed\")]},\n",
    "            \n",
    "            {\"Scaler__use\":         [False],\n",
    "             \"TDA\":                 [gd.representations.ProkhorovDistance()], \n",
    "             \"TDA__coefs\":          np.array([0,1]), \n",
    "             \"Estimator\":           [KNeighborsClassifier(metric=\"precomputed\")]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model is the best estimator found after 3-fold cross-validation of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = GridSearchCV(pipe, param, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is time to train the model. Since we perform cross-validation, the computation can be quite long, especially if using k-NN with bottleneck distances, which is quite time-consuming. You may consider grabbing a cup of coffee at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/base.py\", line 702, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 348, in transform\n",
      "    Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric=\"Prokhorov\", n_jobs=self.n_jobs, coefs=self.coefs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 178, in pairwise_persistence_diagram_distances\n",
      "    return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(prokhorov_distance,  X, Y, **kwargs), n_jobs=n_jobs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 126, in _pairwise\n",
      "    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 143, in flat_metric\n",
      "    return metric(X[int(a[0])], Y[int(b[0])], **kwargs)\n",
      "ValueError: array has incorrect number of dimensions: 0; expected 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/base.py\", line 702, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 348, in transform\n",
      "    Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric=\"Prokhorov\", n_jobs=self.n_jobs, coefs=self.coefs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 178, in pairwise_persistence_diagram_distances\n",
      "    return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(prokhorov_distance,  X, Y, **kwargs), n_jobs=n_jobs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 126, in _pairwise\n",
      "    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 143, in flat_metric\n",
      "    return metric(X[int(a[0])], Y[int(b[0])], **kwargs)\n",
      "ValueError: array has incorrect number of dimensions: 0; expected 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/base.py\", line 702, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 348, in transform\n",
      "    Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric=\"Prokhorov\", n_jobs=self.n_jobs, coefs=self.coefs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 178, in pairwise_persistence_diagram_distances\n",
      "    return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(prokhorov_distance,  X, Y, **kwargs), n_jobs=n_jobs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 126, in _pairwise\n",
      "    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 143, in flat_metric\n",
      "    return metric(X[int(a[0])], Y[int(b[0])], **kwargs)\n",
      "ValueError: array has incorrect number of dimensions: 0; expected 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/base.py\", line 702, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 348, in transform\n",
      "    Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric=\"Prokhorov\", n_jobs=self.n_jobs, coefs=self.coefs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 178, in pairwise_persistence_diagram_distances\n",
      "    return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(prokhorov_distance,  X, Y, **kwargs), n_jobs=n_jobs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 126, in _pairwise\n",
      "    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 143, in flat_metric\n",
      "    return metric(X[int(a[0])], Y[int(b[0])], **kwargs)\n",
      "ValueError: array has incorrect number of dimensions: 0; expected 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/base.py\", line 702, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 348, in transform\n",
      "    Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric=\"Prokhorov\", n_jobs=self.n_jobs, coefs=self.coefs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 178, in pairwise_persistence_diagram_distances\n",
      "    return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(prokhorov_distance,  X, Y, **kwargs), n_jobs=n_jobs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 126, in _pairwise\n",
      "    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 143, in flat_metric\n",
      "    return metric(X[int(a[0])], Y[int(b[0])], **kwargs)\n",
      "ValueError: array has incorrect number of dimensions: 0; expected 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/sklearn/base.py\", line 702, in fit_transform\n",
      "    return self.fit(X, y, **fit_params).transform(X)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 348, in transform\n",
      "    Xfit = pairwise_persistence_diagram_distances(X, self.diagrams_, metric=\"Prokhorov\", n_jobs=self.n_jobs, coefs=self.coefs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 178, in pairwise_persistence_diagram_distances\n",
      "    return _pairwise(pairwise_distances, True, XX, YY, metric=_sklearn_wrapper(prokhorov_distance,  X, Y, **kwargs), n_jobs=n_jobs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 126, in _pairwise\n",
      "    d = par(delayed(metric)([triu[0][i]], [triu[1][i]]) for i in range(len(triu[0])))\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/nihell/.local/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.9/site-packages/gudhi-3.5.0rc1-py3.9-linux-x86_64.egg/gudhi/representations/metrics.py\", line 143, in flat_metric\n",
      "    return metric(X[int(a[0])], Y[int(b[0])], **kwargs)\n",
      "ValueError: array has incorrect number of dimensions: 0; expected 1\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/home/nihell/.local/lib/python3.9/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.4010989  0.4010989  0.22344322 0.22344322 0.34981685 0.34981685\n",
      " 0.1978022  0.22161172 0.14835165 0.14835165 0.14835165 0.14835165\n",
      " 0.75457875 0.22527473        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model.fit(train_dgms, train_labs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is finally over! Let us check what is the best method for persistence diagrams with respect to this classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Estimator': RandomForestClassifier(), 'Scaler__use': True, 'TDA': Landscape(), 'TDA__resolution': 100}\n"
     ]
    }
   ],
   "source": [
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like random forests and landscapes did the best for this small dataset! Let's see our model accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 1.0\n",
      "Test accuracy  = 0.4\n"
     ]
    }
   ],
   "source": [
    "print(\"Train accuracy = \" + str(model.score(train_dgms, train_labs)))\n",
    "print(\"Test accuracy  = \" + str(model.score(test_dgms,  test_labs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70%, not so bad for such a small dataset! The accuracy you get can even improve for bigger datasets and more parameters in the cross-validation (but training time will increase as well ;-)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
